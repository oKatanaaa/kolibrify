## Overview

This example demonstrates how to align (via DPO) a model using kolibrify. It uses a smaller GPT-4 subset of the Dolphin dataset, kindly provided by the cognitivecomputations team. The subset is used as accepted responses. Rejected responses are generated by a model, finetuned on this subset. This alignment strategy is similar to the one described in the [SPIN paper](https://arxiv.org/abs/2401.01335).

### How to Run This Example

Follow these steps to get started:

1. Navigate to the example directory with `cd`.
2. Execute `kolibrify-dpo dolphin-mistral-test.yaml`.
   - If your system is equipped with multiple GPUs, use `CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0 kolibrify-dpo dolphin-mistral-test.yaml`. Substitute `CUDA_VISIBLE_DEVICES` with the correct GPU ID if 0 is in use.
   - To run the training in the background, use `nohup CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0 kolibrify-dpo dolphin-mistral-test.yaml > dolphin-mistral-test.log 2>&1 &`. This command logs all outputs to `dolphin-mistral-test.log` in the current directory.
3. After training, load the model with unsloth as usual.
4. To integrate LoRA adapters with the base model, execute `kolibrify-merge dolphin-mistral-test.yaml`. Include the CUDA environment variables from above if your system has multiple GPUs.
5. You're all set! The model is now ready for use.

### Configuration

The DPO config structure largely follows SFT configuration, the only difference is in the way you declare datasets. Now you need to specify files for `accepted` and `rejected` responses.

> [!NOTE]
> At the moment curriculum functionality is not working for DPO, all of the provided datasets will be mixed together.
